# Agent Benchmark

A framework for benchmarking AI coding agents using OpenCode.

## Overview

This tool enables systematic comparison of how different AI coding agents perform tasks. It runs multiple agents in isolated Docker containers, collects metrics, and uses AI to generate analysis reports.

## Installation

```bash
uv sync
```

## Prerequisites

- Docker
- OpenCode CLI with authenticated providers

### OpenCode Authentication

Authenticate with your preferred providers:

```bash
opencode auth login  # Follow prompts for Anthropic, GitHub Copilot, etc.
opencode auth list   # Verify credentials
```

### Available Models

List available models:

```bash
opencode models
```

Models use the format `provider/model-id`:

| Provider | Example Models |
|----------|----------------|
| `anthropic` | `anthropic/claude-sonnet-4-5`, `anthropic/claude-opus-4-5` |
| `github-copilot` | `github-copilot/claude-sonnet-4.5`, `github-copilot/claude-haiku-4.5` |
| `opencode` | `opencode/gpt-5-nano` (free tier) |

## Usage

### Run an experiment and analyze

```bash
uv run agent-benchmark run-and-analyze experiments/sonnet-vs-haiku.toml
```

This runs all agents, then automatically runs AI analysis on the results.

### Run an experiment only

```bash
uv run agent-benchmark run experiments/sonnet-vs-haiku.toml
```

### Analyze existing results

```bash
uv run agent-benchmark analyze results/sonnet-vs-haiku-2026-02-11-123456/
```

Requires an `[analysis]` section in the experiment config.

### List past experiments

```bash
uv run agent-benchmark list
```

## Configuration

Experiments are configured using TOML files. See `experiments/sonnet-vs-gpt5.toml` for an example.

### Basic Example

```toml
[experiment]
name = "sonnet-vs-haiku"
description = "Compare Claude Sonnet 4.5 and Haiku 4.5"

[target]
repo = "https://github.com/user/test-repo"
# commit = "abc123"  # Optional: pin to specific commit

[prompt]
text = "/speckit.plan 001-todo-crud\n\nCommit your changes."
# Or use a file from the repo:
# file = "PROMPT.md"

[settings]
runs_per_agent = 2
parallel = true
timeout_minutes = 30

[analysis]
model = "anthropic/claude-opus-4-6"
prompt = """
Compare the plans generated by each agent. Consider:
1. Completeness: Did each agent address all spec requirements?
2. Accuracy: Correct interpretation of the spec?
3. Consistency: How much variance between runs of the same agent?
4. Quality: Overall plan quality and feasibility
"""

[[agents]]
id = "sonnet-4.5"
model = "github-copilot/claude-sonnet-4.5"

[[agents]]
id = "haiku-4.5"
model = "github-copilot/claude-haiku-4.5"
```

### Agent Configuration

Each agent requires:
- `id` - Unique identifier for the agent
- `model` - Model in `provider/model-id` format (optional, uses default if not specified)
- `extra_args` - Additional CLI arguments for opencode (optional)

## Docker Image

The benchmark runs agents in Docker containers. Rebuild after code changes:

```bash
just rebuild
```

## Output Structure

Results are saved to `results/<experiment-name>-<timestamp>/`:

```
results/sonnet-vs-haiku-2026-02-11-123456/
├── config.toml           # Copy of experiment config
├── analysis.md           # AI-generated analysis report
├── stats.json            # AI-generated statistics
├── sonnet-4.5-1/         # Run 1 for sonnet agent
│   ├── metrics.json      # Quantitative metrics
│   ├── run.log           # Agent execution log
│   └── repo/             # Repository state after run
├── sonnet-4.5-2/         # Run 2 for sonnet agent
├── haiku-4.5-1/          # Run 1 for haiku agent
└── haiku-4.5-2/          # Run 2 for haiku agent
```

## Architecture

- **cli.py** - Command-line interface (run, analyze, run-and-analyze, list)
- **runner.py** - Orchestrates experiment execution
- **container.py** - Manages Docker containers
- **metrics.py** - Collects quantitative metrics
- **analysis.py** - AI-powered analysis using OpenCode in a container
- **config.py** - Pydantic configuration models
- **display.py** - Rich TUI progress display
