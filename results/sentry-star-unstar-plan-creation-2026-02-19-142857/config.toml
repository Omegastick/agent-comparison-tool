[experiment]
name = "sentry-star-unstar-plan-creation"
description = "Compare plan generation quality on a real-world Sentry feature (star/unstar shared issue views) - derived from PR #87463"

[target]
repo = "https://github.com/Omegastick/sentry"
commit = "001-star-unstar-views"

[prompt]
text = "/speckit.plan 001-star-unstar-views"

[settings]
runs_per_agent = 3
parallel = true
timeout_minutes = 30

[analysis]
model = "github-copilot/claude-opus-4.6"
prompt = """
Compare the plans generated by each agent against the actual implementation in
https://github.com/getsentry/sentry/pull/87463. Consider:
1. Completeness: Did each agent address all spec requirements?
2. Accuracy: Correct interpretation of the spec?
3. Convention adherence: Did plans follow Sentry patterns (endpoint base classes,
   model decorators, serializer registry, factory-based tests)?
4. Consistency: How much variance between runs of the same agent?
5. Quality: Overall plan quality, feasibility, and alignment with the real PR
"""

[[agents]]
id = "sonnet-4.5"
model = "github-copilot/claude-sonnet-4.5"

[[agents]]
id = "sonnet-4.6"
model = "anthropic/claude-sonnet-4-6"

[[agents]]
id = "opus-4.6"
model = "github-copilot/claude-opus-4-6"

[[agents]]
id = "gpt-5-mini"
model = "github-copilot/gpt-5-mini"

