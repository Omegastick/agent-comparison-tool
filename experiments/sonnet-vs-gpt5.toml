[experiment]
name = "sonnet-vs-gpt5"
description = "Compare plan generation quality between Claude Sonnet 4.5 and GPT-5"

[target]
repo = "https://github.com/Omegastick/sample-todo-api"

[prompt]
text = "/speckit.plan 001-todo-crud"

[settings]
runs_per_agent = 2
parallel = true
timeout_minutes = 30

[analysis]
model = "anthropic/claude-opus-4-6"
prompt = """
Compare the plans generated by each agent. Consider:
1. Completeness: Did each agent address all spec requirements?
2. Accuracy: Correct interpretation of the spec?
3. Consistency: How much variance between runs of the same agent?
4. Quality: Overall plan quality and feasibility
"""

[[agents]]
id = "sonnet-4.5"
model = "github-copilot/claude-sonnet-4.5"

[[agents]]
id = "gpt-5"
model = "github-copilot/gpt-5"
