"""Analysis and report generation for benchmark results."""

from pathlib import Path

ANALYSIS_SYSTEM_PROMPT = """You are analyzing AI coding agent benchmark results.

## Directory Structure
Your working directory is /workspace/results which contains:
- config.toml: Original benchmark configuration
- {agent-id}-{run-number}/: Individual run directories containing:
  - metrics.json: Quantitative metrics (duration, exit_code, plan stats)
  - .benchmark/run.log: Agent execution log
  - repo/: Repository state after agent execution
    - .specify/specs/*/plan.md: Generated plan (if created)

## Metrics Available
Each run's metrics.json contains:
- run_id, agent_id: Identifiers
- exit_code: 0 = success
- duration_seconds: Execution time
- plan: total_lines, sections, tasks, files_referenced

## Your Approach
1. First, list all run directories to understand what needs to be analyzed
2. Create a todo list of tasks - one task per run directory to inspect
3. For each run directory:
   - Read metrics.json for quantitative data
   - Read the generated plan if it exists
   - Note key observations
4. After inspecting all runs, synthesize your findings
5. Write the output files, iterating until complete

## Output Files
You must write two files:

1. /workspace/results/analysis.md - A well-structured markdown report with:
   - Executive Summary (2-3 sentences)
   - Agent-by-Agent Analysis (detailed qualitative analysis)
   - Comparative Rankings
   - Notable Observations

2. /workspace/results/stats.json - A JSON file with key statistics:
   - experiment_name: Name from config
   - total_runs: Number of runs
   - agents: Object with per-agent stats (success_rate, avg_duration, etc.)
   - rankings: Ordered list of agents by performance
   - Any other relevant aggregate statistics

## User Criteria
The user will provide specific criteria to evaluate. Focus your analysis on those criteria.
"""


def run_ai_analysis(
    results_path: Path,
    model: str = "anthropic/claude-opus-4-5",
    prompt: str | None = None,
) -> bool:
    """Run OpenCode in a container to analyze results and write analysis.md.

    The agent writes directly to results_path/analysis.md.

    Args:
        results_path: Path to the results directory to analyze.
        model: Model to use for analysis.
        prompt: Custom analysis prompt. If None, uses a default prompt.

    Returns:
        True if analysis succeeded, False otherwise.
    """
    from .container import AnalysisContainerConfig, ContainerManager

    if prompt is None:
        prompt = """Compare the plans generated by each agent. Consider:
1. Completeness: Did each agent address all spec requirements?
2. Accuracy: Correct interpretation of the spec?
3. Consistency: How much variance between runs of the same agent?
4. Quality: Overall plan quality and feasibility"""

    config = AnalysisContainerConfig(
        results_path=results_path.resolve(),
        model=model,
        prompt=prompt,
        system_prompt=ANALYSIS_SYSTEM_PROMPT,
        timeout_seconds=600,
    )

    manager = ContainerManager()
    try:
        result = manager.run_analysis(config)

        if result.error:
            print(f"AI analysis failed: {result.error}")
            return False

        if result.exit_code != 0:
            print(f"AI analysis failed with exit code {result.exit_code}")
            return False

        return True
    except Exception as e:
        print(f"AI analysis failed: {e}")
        return False
    finally:
        manager.cleanup()
